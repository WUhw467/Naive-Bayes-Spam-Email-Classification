{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn string into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textParse(input_string):\n",
    "    listofTokens = re.split(r'\\W+',input_string)\n",
    "    return [tok.lower() for tok in listofTokens if len(listofTokens) > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Vocabulary List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def createVocablist(doclist):\n",
    "    vocabSet = set([])\n",
    "    for document in doclist:\n",
    "        vocabSet = vocabSet|set(document)\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setOfWord2Vec(vocabList, doc):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in doc:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNB(trainMat, trainClass):\n",
    "    numDocs = len(trainMat)\n",
    "    numWords = len(trainMat[0])\n",
    "    p_Spam = trainClass.count('spam')/float(len(trainClass))\n",
    "    p_Ham = 1-p_Spam\n",
    "    Ham_num = np.ones((numWords))\n",
    "    Spam_num = np.ones((numWords))\n",
    "    Ham_denom = 2\n",
    "    Spam_denom = 2\n",
    "\n",
    "    for i in range(numDocs):\n",
    "        if trainClass[i] == 'spam':\n",
    "            Spam_num += trainMat[i]\n",
    "            Spam_denom += sum(trainMat[i])\n",
    "        else:\n",
    "            Ham_num += trainMat[i]\n",
    "            Ham_denom += sum(trainMat[i])\n",
    "    Spam_vec = np.log(Spam_num/Spam_denom)\n",
    "    Ham_vec = np.log(Ham_num/Ham_denom)\n",
    "    return Spam_vec, Ham_vec, p_Spam, p_Ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyNB(wordVec,Spam_vec, Ham_vec, p_Spam, p_Ham):\n",
    "    p1 = np.log(p_Spam) + sum(wordVec*Spam_vec)\n",
    "    p0 = np.log(p_Ham) + sum(wordVec*Ham_vec)\n",
    "    if p0 > p1:\n",
    "        return 'ham'\n",
    "    else:\n",
    "        return 'spam'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spam():\n",
    "    df = pd.read_csv(\"spam.csv\")\n",
    "    msk = np.random.rand(len(df)) < 0.8\n",
    "    train = df[msk]\n",
    "    test = df[~msk]\n",
    "    vocabList = createVocablist([textParse(str) for str in list(df['Message'])])\n",
    "    trainDoc = [textParse(str) for str in list(train['Message'])]\n",
    "    testDoc = [textParse(str) for str in list(test['Message'])]\n",
    "    trainClass = list(train['Category'])\n",
    "    testClass = list(test['Category'])\n",
    "    trainMat = []\n",
    "    for doc in trainDoc:\n",
    "        trainMat.append(setOfWord2Vec(vocabList,doc))\n",
    "    Spam_vec, Ham_vec, p_Spam, p_Ham = trainNB(np.array(trainMat),trainClass)\n",
    "    errorCount = 0\n",
    "    for i in range(len(testDoc)):\n",
    "        wordVec = setOfWord2Vec(vocabList,testDoc[i])\n",
    "        prediction = classifyNB(np.array(wordVec), Spam_vec, Ham_vec, p_Spam, p_Ham)\n",
    "        if testClass[i] != prediction:\n",
    "            errorCount += 1\n",
    "    accuracy = 1 - errorCount/len(testClass)\n",
    "    print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9686907020872866\n"
     ]
    }
   ],
   "source": [
    "spam()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('580')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b8aa885a99db92923b87a60ebfec3254a76fa837dfa343bad65102759a6e4ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
